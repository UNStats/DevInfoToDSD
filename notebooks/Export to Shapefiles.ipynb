{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export DevInfo Table to Shapefiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DevInfo stores the three files (.shp, .shx, and .dbf) in 3 columns in the Access database. This script will execute a query that will return those files, plus an additional column, `Layer_NId`, that will be used to construct the file name. This file name will be referenced by the Export to GeoJSON scripts to include the geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyodbc\n",
    "# [x for x in pyodbc.drivers() if x.startswith('Microsoft Access Driver')]\n",
    "# if you see [], you may need to install the 64-bit ACE drivers\n",
    "# https://github.com/mkleehammer/pyodbc/wiki/Connecting-to-Microsoft-Access\n",
    "# 64bit drivers : https://www.microsoft.com/en-us/download/confirmation.aspx?id=13255\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure your paths and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "access_database = r'Z:\\dev\\nepal-import\\NepalInfo2016_for_python.accdb'\n",
    "output_base = 'nepal'\n",
    "output_shapes_folder = 'shapes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the query against the DevInfo Access tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "connStr = (\n",
    "    r'Driver={{Microsoft Access Driver (*.mdb, *.accdb)}};'\n",
    "    r'DBQ={};'.format(access_database)\n",
    ")\n",
    "\n",
    "cnxn = pyodbc.connect(connStr)\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT UT_Area_Map_Layer.Layer_Shp, UT_Area_Map_Layer.Layer_Shx, UT_Area_Map_Layer.Layer_dbf, UT_Area_Map_Layer.Layer_NId, UT_Area_Map_Metadata_en.Layer_Name, UT_Area_en.Area_Parent_NId, UT_Area_en.Area_ID, UT_Area_en.Area_Level, UT_Area_en.Area_Parent_NId\n",
    "FROM (UT_Area_Map_Layer INNER JOIN UT_Area_Map_Metadata_en ON UT_Area_Map_Layer.Layer_NId = UT_Area_Map_Metadata_en.Layer_NId) INNER JOIN (UT_Area_en INNER JOIN UT_Area_Map ON UT_Area_en.Area_NId = UT_Area_Map.Area_NId) ON UT_Area_Map_Layer.Layer_NId = UT_Area_Map.Layer_NId;\n",
    "\"\"\"\n",
    "crsr = cnxn.execute(sql)\n",
    "\n",
    "# export just a subset for testing\n",
    "# rows = crsr.fetchmany(50)\n",
    "\n",
    "# uncomment this to get crazy and export them all!!\n",
    "rows = crsr.fetchall()\n",
    "\n",
    "print ('sucessfully executed data query :: {} rows returned'.format(len(rows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Shapefiles\n",
    "Loop through each row in the result set and write out each of the data in the columns to the three files needed for a Shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row_count = len(rows)\n",
    "\n",
    "layer_checks = []\n",
    "\n",
    "for i, row in enumerate(rows):\n",
    "    \n",
    "    shp = row[0]\n",
    "    shx = row[1]\n",
    "    dbf = row[2]\n",
    "\n",
    "    layer_id = row[3]\n",
    "\n",
    "    # We only need to write out one shapefile per layer, so we can skip any duplicates by checking for already created layers\n",
    "    if layer_id in layer_checks:\n",
    "        continue\n",
    "\n",
    "    layer_checks.append(layer_id)\n",
    "\n",
    "    out_file = '{}/{}'.format(os.path.join(output_base, output_shapes_folder), layer_id)\n",
    "\n",
    "    print ('adding {} of {}'.format(i+1, row_count))\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    with open('{}.shp'.format(out_file), 'wb') as writer:\n",
    "        writer.write(shp)\n",
    "    with open('{}.shx'.format(out_file), 'wb') as writer:\n",
    "        writer.write(shx)\n",
    "    with open('{}.dbf'.format(out_file), 'wb') as writer:\n",
    "        writer.write(dbf)\n",
    "        \n",
    "    # here we are also writing a GCS WGS 1984 .prj file that will define the spatial reference of each shapefile\n",
    "    with open('{}.prj'.format(out_file), 'w') as writer:\n",
    "        writer.write('GEOGCS[\"GCS_WGS_1984\",DATUM[\"D_WGS_1984\",SPHEROID[\"WGS_1984\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.017453292519943295]]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
