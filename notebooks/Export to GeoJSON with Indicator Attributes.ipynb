{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to GeoJSON with Indicator Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook will walk you through the process of converting your DevInfo database into the possible output formats of:\n",
    "- GeoJSON with only geometry and a reference area id\n",
    "- GeoJSON with full attributes AND geomery\n",
    "\n",
    "To export the resulting GeoJSON files to a File Geodatabase, follow the instructions in the **Export GeoJSON to File Geodatabase** notebook.\n",
    "\n",
    "### **Important**\n",
    "\n",
    "You need to have previously run the Export to Shapefiles notebook **before** running this notebook as it will need a folder location of the individual shapefiles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the needed python libraries\n",
    "install the GDAL pythyon library (ogr import below) by opening the anaconda prompt and using `conda install gdal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import csv\n",
    "import pyodbc\n",
    "from osgeo import ogr\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# location of access database\n",
    "access_database = r'Z:\\dev\\nepal-import\\NepalInfo2016_for_python.accdb'\n",
    "\n",
    "# where is your working folder? usually just the name of the country you are working on\n",
    "output_base = 'nepal'\n",
    "\n",
    "# location of individual shapefiles for each layer. \n",
    "# this is created by running the Export to Shapefiles jupyter notebook, see *Important* note above\n",
    "output_shapes_folder = 'shapes'\n",
    "\n",
    "# where do you want to write the output geojson files?\n",
    "output_geojson_folder = 'geojson_full'\n",
    "\n",
    "# flag for full properties & geometries\n",
    "# True for full attributes and geometry\n",
    "# False for only REF_AREA and REF_AREA_ID and geometry\n",
    "full_props = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for the `Esri Shapefile` driver\n",
    "The DevInfo Access database stores the geometry as a Shapefile. We can use this driver to read that into our script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shp_driver_lbl = 'Esri Shapefile'\n",
    "shp_driver = ogr.GetDriverByName(shp_driver_lbl)\n",
    "if shp_driver is None:\n",
    "    print ('{} driver not available.'.format(shp_driver_lbl))\n",
    "else:\n",
    "    print ('{} driver IS available.'.format(shp_driver_lbl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Choose your input dataset**\n",
    "If you are going to use a CSV file as your input, continue with **[Option 1]**. \n",
    "\n",
    "If you are going to query the Access Database directly, you can skip to the first cell marked **[Option 2]**.\n",
    "  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Option 1] Use a CSV file as your input\n",
    "Instead of querying the access database, you can specify a CSV to use as your input.\n",
    "\n",
    "This can be the output of the **Export All Data to Single CSV** notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory = r'C:\\Users\\adam\\devinfo\\tanzania'\n",
    "csv_file =  r'devinfo_output_tz.csv'\n",
    "csv_file_path = os.path.join(working_directory, csv_file)\n",
    "\n",
    "# set encoding \n",
    "# when reading some CSV files, there may be encoding issues that result in unicode characters appearing in the field names\n",
    "# if you have any unexpected behavior that is related to this, either re-save your file with \"utf-8\" encoding\n",
    "# or try using 'utf-8-sig' as your encoding value below\n",
    "# Stack Overflow reference: https://stackoverflow.com/questions/17912307/u-ufeff-in-python-string/17912811#17912811\n",
    "encoding = 'utf-8'\n",
    "with open(csv_file_path, encoding=encoding) as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    rows = list(reader)\n",
    "\n",
    "print ('done reading rows from CSV.')\n",
    "\n",
    "# print the first row of data to validate that the header rows were successfully parsed w/o encoding issues\n",
    "print (rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Option 2] Query the DevInfo Access Database\n",
    "Query the DevInfo Access database directly for the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "connStr = (\n",
    "    r'Driver={{Microsoft Access Driver (*.mdb, *.accdb)}};'\n",
    "    r'DBQ={};'.format(access_database)\n",
    ")\n",
    "\n",
    "cnxn = pyodbc.connect(connStr)\n",
    "\n",
    "sql = \"\"\"\\\n",
    "SELECT\n",
    "UT_Data.Indicator_NId AS INDICATOR_ID, \n",
    "UT_Indicator_en.Indicator_Name AS INDICATOR, \n",
    "UT_Data.Data_Value AS OBS_VALUE, \n",
    "UT_Unit_en.Unit_NId AS UNIT_ID, \n",
    "UT_Unit_en.Unit_Name AS UNIT, \n",
    "UT_Area_en.Area_ID AS REF_AREA_ID, \n",
    "UT_Area_en.Area_Name AS REF_AREA, \n",
    "UT_TimePeriod.TimePeriod AS TIME_PERIOD,\n",
    "\n",
    "UT_Indicator_Classifications_en.Publisher AS PUBLISHER,\n",
    "UT_Area_Map_Layer.Layer_NId AS LAYER_ID,\n",
    "\n",
    "UT_Area_Level_en.Area_Level_Name AS AREA_LEVEL_NAME, \n",
    "UT_Area_en.Area_Level AS AREA_LEVEL,\n",
    "UT_Indicator_Classifications_en.IC_Name AS IC_NAME, \n",
    "UT_Subgroup_Vals_en.Subgroup_Val AS SUBGROUP_VAL, \n",
    "UT_Subgroup_Type_en.Subgroup_Type_Name AS SUBGROUP_TYPE_NAME\n",
    "\n",
    "FROM \n",
    "((((UT_Area_Map_Layer INNER JOIN ((UT_Area_Level_en INNER JOIN (UT_Subgroup_Vals_en INNER JOIN (UT_Unit_en INNER JOIN (UT_Indicator_en INNER JOIN (UT_Indicator_Unit_Subgroup INNER JOIN (UT_TimePeriod INNER JOIN (UT_Indicator_Classifications_en INNER JOIN (UT_Area_en INNER JOIN UT_Data ON UT_Area_en.[Area_NId] = UT_Data.[Area_NId]) ON UT_Indicator_Classifications_en.IC_NId = UT_Data.Source_NId) ON UT_TimePeriod.TimePeriod_NId = UT_Data.TimePeriod_NId) ON UT_Indicator_Unit_Subgroup.IUSNId = UT_Data.IUSNId) ON UT_Indicator_en.Indicator_NId = UT_Indicator_Unit_Subgroup.Indicator_NId) ON UT_Unit_en.Unit_NId = UT_Indicator_Unit_Subgroup.Unit_NId) ON UT_Subgroup_Vals_en.Subgroup_Val_NId = UT_Indicator_Unit_Subgroup.Subgroup_Val_NId) ON UT_Area_Level_en.Area_Level = UT_Area_en.Area_Level) INNER JOIN UT_Area_Map ON UT_Area_en.Area_NId = UT_Area_Map.Area_NId) ON UT_Area_Map_Layer.Layer_NId = UT_Area_Map.Layer_NId) INNER JOIN UT_Area_Map_Metadata_en ON UT_Area_Map_Layer.Layer_NId = UT_Area_Map_Metadata_en.Layer_NId) INNER JOIN UT_Subgroup_Vals_Subgroup ON UT_Subgroup_Vals_en.Subgroup_Val_NId = UT_Subgroup_Vals_Subgroup.Subgroup_Val_NId) INNER JOIN UT_Subgroup_en ON UT_Subgroup_Vals_Subgroup.Subgroup_NId = UT_Subgroup_en.Subgroup_NId) INNER JOIN UT_Subgroup_Type_en ON UT_Subgroup_en.Subgroup_Type = UT_Subgroup_Type_en.Subgroup_Type_NId\n",
    "\n",
    "ORDER BY \n",
    "UT_Data.Indicator_NId\n",
    "\"\"\"\n",
    "\n",
    "crsr = cnxn.execute(sql)\n",
    "\n",
    "rows = crsr.fetchall()\n",
    "\n",
    "print ('sucessfully executed data query :: {} rows returned'.format(len(rows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prepare to query the DevInfo Access Database\n",
    "Here we will map our DevInfo fields to the DSD as defined here. **TODO :: add link(s) to reference data schema**\n",
    "\n",
    "This is our ouptut data schema that will show in the GeoJSON `properties`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "field_mappings = [\n",
    "    'INDICATOR_ID',\n",
    "    'INDICATOR',\n",
    "    'REF_AREA',\n",
    "    'REF_AREA_ID',\n",
    "    'OBS_VALUE',\n",
    "    'UNIT_ID',\n",
    "    'UNIT',\n",
    "    'TIME_PERIOD'\n",
    "]\n",
    "\n",
    "add_fields = [\n",
    "    'PUBLISHER',\n",
    "    'LAYER_ID'\n",
    "]\n",
    "\n",
    "field_mappings = field_mappings + add_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk up the data\n",
    "Chunk up the data by Indicator. This will let us create one layer per Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunks = {}\n",
    "for row in rows:\n",
    "    \n",
    "    ind_id = row['INDICATOR_ID']\n",
    "    layer_id = row['LAYER_ID']\n",
    "    \n",
    "    if ind_id not in chunks:\n",
    "        chunks[ind_id] = {}\n",
    "    \n",
    "    if layer_id not in chunks[ind_id]:\n",
    "        chunks[ind_id][layer_id] = {}\n",
    "        chunks[ind_id][layer_id]['rows'] = []\n",
    "    \n",
    "    chunks[ind_id][layer_id]['rows'].append(row)\n",
    "\n",
    "print ('done chunking data by indicator, by layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the Attribute Data & Spatial Data\n",
    "Finally, we will step through our data to create individual geojson files for each layer, for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store the already reference geometry in memory for faster recall\n",
    "geom_cache = {}\n",
    "\n",
    "for c in chunks:\n",
    "    \n",
    "    # test with just one indicator\n",
    "    if c != '1.0':\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    ind = c\n",
    "    lyrs = chunks[c]\n",
    "    \n",
    "    for lyr in lyrs:\n",
    "        \n",
    "        feature_collection = {\n",
    "            'type' : 'FeatureCollection',\n",
    "            'features': []\n",
    "        }\n",
    "    \n",
    "        rows = lyrs[lyr]['rows']\n",
    "        for row in rows:\n",
    "            layer_id = row['LAYER_ID']\n",
    "            area_id = row['REF_AREA_ID']\n",
    "            \n",
    "            # test with just one layer\n",
    "            if layer_id != '270':\n",
    "                continue\n",
    "            \n",
    "            geom = None\n",
    "            if full_props:\n",
    "                if area_id not in geom_cache:\n",
    "                    # look for the already created shapefile\n",
    "                    shp_file_path = '{}/{}.shp'.format(os.path.join(output_base, output_shapes_folder), layer_id)\n",
    "\n",
    "                    # check to see if we were able to get the shapefile\n",
    "                    # TODO: add logging rather than just printing an exception\n",
    "                    shp_file = shp_driver.Open(shp_file_path)\n",
    "                    if shp_file is None:\n",
    "                        print ('{} shape file not found'.format(shp_file_path))\n",
    "                        continue\n",
    "\n",
    "                    layer = shp_file.GetLayer()\n",
    "                    \n",
    "                    where_clause = 'ID_ = \\'{}\\''.format(area_id)\n",
    "                    layer.SetAttributeFilter(where_clause)\n",
    "                    feature = layer.GetNextFeature()\n",
    "                    if feature is None:\n",
    "                        print ('unable to get feature for layer {} :: where {}'.format(layer_id, where_clause))\n",
    "                        continue\n",
    "                        \n",
    "                    geom_ref = feature.GetGeometryRef()\n",
    "\n",
    "                    geom_json_str = geom_ref.ExportToJson()\n",
    "\n",
    "                    geom = json.loads(geom_json_str)\n",
    "\n",
    "                    #store in cache\n",
    "                    geom_cache[area_id] = geom\n",
    "\n",
    "                    # clean up\n",
    "                    del feature\n",
    "                    del layer\n",
    "                    del shp_file\n",
    "\n",
    "                else:\n",
    "                    geom = geom_cache[area_id]\n",
    "    \n",
    "            # setup the new feature\n",
    "            feature = {\n",
    "                'type': 'Feature',\n",
    "                'properties': {},\n",
    "                'geometry': geom\n",
    "            }\n",
    "            \n",
    "            # if we are going to include full attributes and geometry\n",
    "            if full_props:\n",
    "                for field in field_mappings:\n",
    "                    feature['properties'][field] = row[field]\n",
    "            else:\n",
    "                feature['properties']['REF_AREA'] = row['REF_AREA']\n",
    "                feature['properties']['REF_AREA_ID'] = row['REF_AREA_ID']\n",
    "\n",
    "            feature_collection['features'].append(feature)\n",
    "        \n",
    "        # create filename for output geojson file\n",
    "        new_ind = str(c)\n",
    "        new_ind_id = new_ind.replace('.', '_')\n",
    "        layer_name = 'indicator_{}_layer_{}.geojson'.format(new_ind_id, layer_id)\n",
    "\n",
    "        # full path for the output geojson file\n",
    "        full_path = Path(os.path.join(output_base, output_geojson_folder, layer_name))    \n",
    "    \n",
    "        print ('writing {} features to {}'.format(len(feature_collection['features']), layer_name))\n",
    "        with open(full_path, 'w') as file:\n",
    "            file.write(json.dumps(feature_collection))\n",
    "\n",
    "del geom_cache\n",
    "print ('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
